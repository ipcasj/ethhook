# EthHook Implementation Roadmap: Fast ‚Üí Cheap

**Date**: November 22, 2025  
**Strategy**: Rust unified pipeline NOW (2-3 weeks) ‚Üí C migration LATER (after validation)

---

## üéØ Overall Strategy

```
Phase 1 (Weeks 1-3): Rust Unified Pipeline
  ‚îú‚îÄ Fix production crisis (29-minute queries)
  ‚îú‚îÄ Validate product-market fit
  ‚îú‚îÄ Deploy production-grade Rust
  ‚îî‚îÄ Monitor for Rust-specific issues
  
Phase 2 (Months 4-6): Validation & Growth
  ‚îú‚îÄ Gather metrics (memory, latency, costs)
  ‚îú‚îÄ Acquire customers and revenue
  ‚îú‚îÄ Identify bottlenecks
  ‚îî‚îÄ Build C expertise on team
  
Phase 3 (Months 7-10): C Migration
  ‚îú‚îÄ Implement modern C unified pipeline
  ‚îú‚îÄ A/B test Rust vs C (stability + costs)
  ‚îú‚îÄ Gradual cutover (10% ‚Üí 100%)
  ‚îî‚îÄ Achieve 62% cost reduction
```

---

## üìÖ PHASE 1: Rust Unified Pipeline (WEEKS 1-3)

### Goal
Fix production crisis, validate product, deploy stable Rust service with **Cloudflare-style defect prevention**.

### Week 1: Core Implementation

#### Day 1-2: Project Setup
```bash
# Create unified pipeline crate
cargo new --bin crates/pipeline
cd crates/pipeline

# Update Cargo.toml
cat >> Cargo.toml <<EOF
[dependencies]
tokio = { version = "1.47", features = ["full"] }
tokio-util = "0.7"
axum = "0.8"
tower = "0.5"
tower-http = "0.6"
sqlx = { version = "0.8.6", features = ["runtime-tokio-native-tls", "postgres"] }
redis = { version = "0.24", features = ["tokio-comp", "connection-manager"] }
reqwest = { version = "0.12", features = ["json"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
anyhow = "1.0"
thiserror = "1.0"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
prometheus = "0.13"
hmac = "0.12"
sha2 = "0.10"
hex = "0.4"

# Local dependencies
common = { path = "../common" }
domain = { path = "../domain" }
config = { path = "../config" }

[dev-dependencies]
mockito = "1.2"
testcontainers = "0.15"
EOF
```

**Architecture**:
```rust
// crates/pipeline/src/main.rs
use tokio::sync::mpsc;

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    // 1. Setup channels (replace Redis)
    let (event_tx, event_rx) = mpsc::channel(10000);
    let (delivery_tx, delivery_rx) = mpsc::channel(10000);
    
    // 2. Spawn WebSocket listeners (4 chains)
    spawn_websocket_listeners(event_tx).await?;
    
    // 3. Batch processor (events ‚Üí endpoints)
    spawn_batch_processor(event_rx, delivery_tx).await?;
    
    // 4. HTTP delivery workers (50 concurrent)
    spawn_delivery_workers(delivery_rx, 50).await?;
    
    Ok(())
}
```

#### Day 3-4: WebSocket Integration
```rust
// crates/pipeline/src/websocket.rs
use tokio::sync::mpsc::Sender;

pub async fn spawn_websocket_listeners(
    event_tx: Sender<Event>
) -> anyhow::Result<()> {
    // Reuse existing event-ingestor code
    let chains = vec!["ethereum", "arbitrum", "optimism", "base"];
    
    for chain in chains {
        let tx = event_tx.clone();
        tokio::spawn(async move {
            loop {
                // WebSocket connection
                match connect_to_chain(chain).await {
                    Ok(mut stream) => {
                        while let Some(event) = stream.next().await {
                            // Send to channel (non-blocking)
                            if tx.send(event).await.is_err() {
                                break; // Channel closed
                            }
                        }
                    }
                    Err(e) => {
                        log::error!("Chain {} error: {}", chain, e);
                        tokio::time::sleep(Duration::from_secs(5)).await;
                    }
                }
            }
        });
    }
    
    Ok(())
}
```

#### Day 5-7: Batch Processor (CRITICAL for performance)
```rust
// crates/pipeline/src/batch.rs
use tokio::sync::mpsc::{Receiver, Sender};
use sqlx::PgPool;

pub async fn spawn_batch_processor(
    mut event_rx: Receiver<Event>,
    delivery_tx: Sender<DeliveryJob>,
    pool: PgPool,
) -> anyhow::Result<()> {
    tokio::spawn(async move {
        let mut batch = Vec::with_capacity(100);
        let mut interval = tokio::time::interval(Duration::from_millis(100));
        
        loop {
            tokio::select! {
                // Collect events into batch
                Some(event) = event_rx.recv() => {
                    batch.push(event);
                    
                    // Process when batch is full
                    if batch.len() >= 100 {
                        process_batch(&batch, &delivery_tx, &pool).await;
                        batch.clear();
                    }
                }
                
                // Process partial batch every 100ms
                _ = interval.tick() => {
                    if !batch.is_empty() {
                        process_batch(&batch, &delivery_tx, &pool).await;
                        batch.clear();
                    }
                }
            }
        }
    });
    
    Ok(())
}

async fn process_batch(
    events: &[Event],
    delivery_tx: &Sender<DeliveryJob>,
    pool: &PgPool,
) {
    // SINGLE QUERY instead of N queries (10-100x faster!)
    let contract_addresses: Vec<String> = events
        .iter()
        .map(|e| e.contract_address.clone())
        .collect();
    
    // Batch query all matching endpoints
    let endpoints = sqlx::query_as!(
        Endpoint,
        r#"
        SELECT DISTINCT e.*
        FROM endpoints e
        WHERE e.is_active = true
          AND (
            e.contract_address = ANY($1)
            OR e.contract_address IS NULL
          )
        "#,
        &contract_addresses
    )
    .fetch_all(pool)
    .await
    .expect("Database query failed"); // NEVER use expect in production!
    
    // Create delivery jobs
    for event in events {
        for endpoint in &endpoints {
            if matches_endpoint(event, endpoint) {
                let job = DeliveryJob {
                    event: event.clone(),
                    endpoint: endpoint.clone(),
                };
                
                // Non-blocking send
                let _ = delivery_tx.try_send(job);
            }
        }
    }
}
```

**‚ö†Ô∏è RUST PRODUCTION SAFETY RULES** (learned from Cloudflare):

```rust
// ‚ùå NEVER DO THIS (Cloudflare mistake):
let feature_vector = FeatureVector::new(&features).unwrap();

// ‚úÖ ALWAYS DO THIS:
let feature_vector = match FeatureVector::new(&features) {
    Ok(v) => v,
    Err(e) => {
        log::error!("Feature vector creation failed: {}", e);
        return; // Graceful degradation
    }
};

// ‚úÖ OR USE expect() WITH CONTEXT:
let feature_vector = FeatureVector::new(&features)
    .expect("Feature vector creation failed - this should never happen");

// ‚úÖ OR RETURN ERROR:
let feature_vector = FeatureVector::new(&features)
    .context("Failed to create feature vector")?;
```

### Week 2: HTTP Delivery & Error Handling

#### Day 8-10: HTTP Workers
```rust
// crates/pipeline/src/delivery.rs
use tokio::sync::mpsc::Receiver;
use reqwest::Client;

pub async fn spawn_delivery_workers(
    mut job_rx: Receiver<DeliveryJob>,
    concurrency: usize,
) -> anyhow::Result<()> {
    let client = Client::builder()
        .timeout(Duration::from_secs(30))
        .pool_max_idle_per_host(50)
        .build()?;
    
    // Spawn worker pool
    for worker_id in 0..concurrency {
        let mut rx = job_rx.clone();
        let client = client.clone();
        
        tokio::spawn(async move {
            while let Some(job) = rx.recv().await {
                // ‚úÖ PRODUCTION-SAFE ERROR HANDLING
                if let Err(e) = deliver_webhook(&job, &client).await {
                    log::error!(
                        "Worker {} delivery failed: endpoint={}, error={}",
                        worker_id,
                        job.endpoint.url,
                        e
                    );
                    
                    // Record failure (don't panic!)
                    record_delivery_failure(&job, e).await;
                }
            }
        });
    }
    
    Ok(())
}

async fn deliver_webhook(
    job: &DeliveryJob,
    client: &Client,
) -> Result<(), DeliveryError> {
    // Generate HMAC signature
    let payload = serde_json::to_vec(&job.event)?;
    let signature = generate_hmac(&payload, &job.endpoint.hmac_secret);
    
    // Send HTTP POST
    let response = client
        .post(&job.endpoint.url)
        .header("X-Webhook-Signature", signature)
        .json(&job.event)
        .send()
        .await?;
    
    if response.status().is_success() {
        log::info!("Webhook delivered: {}", job.endpoint.url);
        Ok(())
    } else {
        Err(DeliveryError::HttpError(response.status()))
    }
}
```

#### Day 11-12: Circuit Breaker (prevent cascading failures)
```rust
// crates/pipeline/src/circuit_breaker.rs
use std::sync::Arc;
use tokio::sync::RwLock;

pub struct CircuitBreaker {
    state: Arc<RwLock<CircuitState>>,
    failure_threshold: u32,
    timeout: Duration,
}

enum CircuitState {
    Closed { failures: u32 },
    Open { opened_at: Instant },
    HalfOpen,
}

impl CircuitBreaker {
    pub async fn call<F, T>(&self, f: F) -> Result<T, CircuitBreakerError>
    where
        F: Future<Output = Result<T, DeliveryError>>,
    {
        // Check state
        let state = self.state.read().await;
        match *state {
            CircuitState::Open { opened_at } => {
                if opened_at.elapsed() > self.timeout {
                    // Try to close circuit
                    drop(state);
                    let mut state = self.state.write().await;
                    *state = CircuitState::HalfOpen;
                } else {
                    return Err(CircuitBreakerError::Open);
                }
            }
            _ => {}
        }
        drop(state);
        
        // Execute function
        match f.await {
            Ok(result) => {
                // Success - close circuit
                let mut state = self.state.write().await;
                *state = CircuitState::Closed { failures: 0 };
                Ok(result)
            }
            Err(e) => {
                // Failure - increment counter
                let mut state = self.state.write().await;
                match *state {
                    CircuitState::Closed { failures } => {
                        let new_failures = failures + 1;
                        if new_failures >= self.failure_threshold {
                            *state = CircuitState::Open {
                                opened_at: Instant::now(),
                            };
                        } else {
                            *state = CircuitState::Closed {
                                failures: new_failures,
                            };
                        }
                    }
                    CircuitState::HalfOpen => {
                        *state = CircuitState::Open {
                            opened_at: Instant::now(),
                        };
                    }
                    _ => {}
                }
                Err(CircuitBreakerError::RequestFailed(e))
            }
        }
    }
}
```

#### Day 13-14: Testing & Validation
```rust
// tests/integration_test.rs
#[tokio::test]
async fn test_unified_pipeline_end_to_end() {
    // Setup test database
    let pool = setup_test_db().await;
    
    // Create test endpoint
    let endpoint = create_test_endpoint(&pool).await;
    
    // Create channels
    let (event_tx, event_rx) = mpsc::channel(100);
    let (delivery_tx, delivery_rx) = mpsc::channel(100);
    
    // Spawn components
    spawn_batch_processor(event_rx, delivery_tx, pool.clone());
    
    // Send test event
    let event = Event {
        chain_id: 1,
        contract_address: endpoint.contract_address.clone(),
        transaction_hash: "0x123...".to_string(),
        block_number: 12345,
        event_data: json!({"test": "data"}),
    };
    
    event_tx.send(event).await.unwrap();
    
    // Verify delivery job created
    let job = tokio::time::timeout(
        Duration::from_secs(1),
        delivery_rx.recv()
    ).await.unwrap().unwrap();
    
    assert_eq!(job.endpoint.url, endpoint.url);
}

#[tokio::test]
async fn test_batch_processing_performance() {
    let pool = setup_test_db().await;
    let (event_tx, event_rx) = mpsc::channel(10000);
    let (delivery_tx, delivery_rx) = mpsc::channel(10000);
    
    spawn_batch_processor(event_rx, delivery_tx, pool.clone());
    
    // Send 1000 events
    let start = Instant::now();
    for i in 0..1000 {
        let event = create_test_event(i);
        event_tx.send(event).await.unwrap();
    }
    
    // Wait for processing
    tokio::time::sleep(Duration::from_millis(200)).await;
    
    let elapsed = start.elapsed();
    
    // Should process 1000 events in <500ms
    assert!(elapsed < Duration::from_millis(500));
}
```

### Week 3: Deployment & Monitoring

#### Day 15-17: Production Deployment
```yaml
# docker-compose.unified.yml
version: '3.8'

services:
  pipeline:
    build:
      context: .
      dockerfile: Dockerfile.pipeline
    container_name: ethhook-pipeline
    restart: unless-stopped
    environment:
      - RUST_LOG=info,pipeline=debug
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}  # Still used for metrics
      - ETHEREUM_WS=${ETHEREUM_WS}
      - ARBITRUM_WS=${ARBITRUM_WS}
      - OPTIMISM_WS=${OPTIMISM_WS}
      - BASE_WS=${BASE_WS}
    ports:
      - "3000:3000"  # Metrics endpoint
    depends_on:
      postgres:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 100M  # Should use ~80MB
        reservations:
          memory: 50M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
  
  admin-api:
    # Keep existing admin API
    image: ethhook-admin-api:latest
    ports:
      - "8080:8080"
    
  postgres:
    image: postgres:15-alpine
    # ... existing config
    
  redis:
    image: redis:7-alpine
    # ... existing config (used for metrics only)
```

```dockerfile
# Dockerfile.pipeline
FROM rust:1.75-alpine AS builder

WORKDIR /app
COPY . .

# Build with optimizations
RUN cargo build --release --bin pipeline

FROM alpine:3.19

RUN apk add --no-cache ca-certificates

COPY --from=builder /app/target/release/pipeline /usr/local/bin/pipeline

EXPOSE 3000

CMD ["pipeline"]
```

#### Day 18-19: Monitoring & Alerting
```rust
// crates/pipeline/src/metrics.rs
use prometheus::{
    IntCounterVec, HistogramVec, Registry, Encoder, TextEncoder,
};

lazy_static! {
    static ref EVENTS_RECEIVED: IntCounterVec = IntCounterVec::new(
        Opts::new("events_received_total", "Total events received"),
        &["chain"]
    ).unwrap();
    
    static ref WEBHOOKS_DELIVERED: IntCounterVec = IntCounterVec::new(
        Opts::new("webhooks_delivered_total", "Total webhooks delivered"),
        &["status"]
    ).unwrap();
    
    static ref BATCH_PROCESSING_TIME: HistogramVec = HistogramVec::new(
        HistogramOpts::new("batch_processing_seconds", "Batch processing time"),
        &["size"]
    ).unwrap();
    
    static ref DELIVERY_LATENCY: HistogramVec = HistogramVec::new(
        HistogramOpts::new("delivery_latency_seconds", "Webhook delivery latency"),
        &["endpoint"]
    ).unwrap();
}

pub fn init_metrics() {
    let registry = Registry::new();
    registry.register(Box::new(EVENTS_RECEIVED.clone())).unwrap();
    registry.register(Box::new(WEBHOOKS_DELIVERED.clone())).unwrap();
    registry.register(Box::new(BATCH_PROCESSING_TIME.clone())).unwrap();
    registry.register(Box::new(DELIVERY_LATENCY.clone())).unwrap();
}

// Metrics endpoint
pub async fn metrics_handler() -> impl IntoResponse {
    let encoder = TextEncoder::new();
    let metric_families = prometheus::gather();
    let mut buffer = Vec::new();
    encoder.encode(&metric_families, &mut buffer).unwrap();
    
    Response::builder()
        .header("Content-Type", "text/plain; version=0.0.4")
        .body(Body::from(buffer))
        .unwrap()
}
```

#### Day 20-21: Gradual Rollout
```bash
#!/bin/bash
# deploy-unified.sh

set -e

echo "üöÄ Deploying unified pipeline..."

# Step 1: Deploy new service (0% traffic)
ssh root@104.248.15.178 <<'EOF'
cd ~/ethhook
docker compose -f docker-compose.unified.yml up -d pipeline
sleep 10
docker logs ethhook-pipeline --tail 50
EOF

# Step 2: Monitor for 30 minutes
echo "‚è≥ Monitoring for 30 minutes..."
sleep 1800

# Step 3: Route 10% traffic
echo "üìä Routing 10% traffic to unified pipeline..."
ssh root@104.248.15.178 <<'EOF'
# Update NGINX config
cat > /etc/nginx/conf.d/ethhook.conf <<'NGINX'
upstream pipeline {
    server 127.0.0.1:3000 weight=1;  # Unified pipeline (10%)
    server 127.0.0.1:8081 weight=9;  # Old services (90%)
}
NGINX

nginx -t && systemctl reload nginx
EOF

# Step 4: Monitor metrics
echo "üìà Check metrics at http://104.248.15.178:3000/metrics"
echo "Compare with old services"

# Step 5: Gradual increase (manual approval)
read -p "Traffic looks good? Increase to 50%? (y/n) " -n 1 -r
if [[ $REPLY =~ ^[Yy]$ ]]; then
    # Increase to 50%
    # ...
fi
```

---

## üõ°Ô∏è RUST PRODUCTION SAFETY CHECKLIST

### Critical Rules (learned from Cloudflare outage)

**1. NEVER use `.unwrap()` in production code**
```rust
// ‚ùå BAD (Cloudflare mistake)
let data = parse_config().unwrap();

// ‚úÖ GOOD
let data = parse_config()
    .context("Failed to parse config")?;

// ‚úÖ ALSO GOOD (with logging)
let data = match parse_config() {
    Ok(d) => d,
    Err(e) => {
        log::error!("Config parse failed: {}", e);
        return default_config(); // Graceful fallback
    }
};
```

**2. Always validate external input**
```rust
// ‚ùå BAD (assumes input is valid)
const MAX_FEATURES: usize = 200;
assert!(features.len() <= MAX_FEATURES); // Can panic!

// ‚úÖ GOOD (graceful handling)
const MAX_FEATURES: usize = 200;
if features.len() > MAX_FEATURES {
    log::warn!(
        "Feature count {} exceeds max {}, truncating",
        features.len(),
        MAX_FEATURES
    );
    features.truncate(MAX_FEATURES);
}
```

**3. Implement circuit breakers**
```rust
// Prevent cascading failures
let circuit_breaker = CircuitBreaker::new(
    5,                              // failures before open
    Duration::from_secs(60),        // timeout before retry
);

circuit_breaker.call(|| {
    deliver_webhook(&endpoint)
}).await?;
```

**4. Use timeouts everywhere**
```rust
// ‚ùå BAD (can hang forever)
let response = client.get(url).send().await?;

// ‚úÖ GOOD (timeout prevents hanging)
let response = tokio::time::timeout(
    Duration::from_secs(30),
    client.get(url).send()
).await??;
```

**5. Monitor tokio runtime health**
```rust
// Detect deadlocks and stalls
tokio::spawn(async {
    let mut interval = tokio::time::interval(Duration::from_secs(10));
    loop {
        interval.tick().await;
        
        let metrics = tokio::runtime::Handle::current().metrics();
        
        log::info!(
            "Tokio runtime: workers={}, active={}, idle={}",
            metrics.num_workers(),
            metrics.num_alive_tasks(),
            metrics.num_idle_blocking_threads()
        );
        
        // Alert if tasks pile up
        if metrics.num_alive_tasks() > 10000 {
            log::error!("Too many active tasks! Possible deadlock?");
        }
    }
});
```

**6. Graceful shutdown**
```rust
#[tokio::main]
async fn main() -> anyhow::Result<()> {
    // Setup signal handler
    let (shutdown_tx, mut shutdown_rx) = tokio::sync::broadcast::channel(1);
    
    tokio::spawn(async move {
        tokio::signal::ctrl_c().await.ok();
        log::info!("Shutdown signal received");
        shutdown_tx.send(()).ok();
    });
    
    // Run services
    tokio::select! {
        _ = run_pipeline() => {
            log::error!("Pipeline exited unexpectedly");
        }
        _ = shutdown_rx.recv() => {
            log::info!("Shutting down gracefully...");
            
            // Drain channels
            tokio::time::sleep(Duration::from_secs(5)).await;
        }
    }
    
    Ok(())
}
```

---

## üìä PHASE 2: Validation & Growth (MONTHS 4-6)

### Goals
- Validate product-market fit
- Gather performance metrics
- Acquire customers and revenue
- Build C expertise

### Month 4: Metrics Collection

**Track these metrics**:
```rust
// Memory usage
let memory_mb = get_process_memory() / 1024 / 1024;
log::info!("Memory usage: {}MB", memory_mb);
// Target: <100MB (currently ~80MB)

// Latency (P50, P95, P99)
let latency_p50 = histogram.percentile(50.0);
let latency_p95 = histogram.percentile(95.0);
let latency_p99 = histogram.percentile(99.0);
log::info!("Latency: P50={}ms P95={}ms P99={}ms", 
    latency_p50, latency_p95, latency_p99);
// Target: P95 <10ms, P99 <50ms

// Throughput
let events_per_sec = total_events / elapsed_secs;
log::info!("Throughput: {} events/sec", events_per_sec);
// Target: >10K events/sec

// Error rate
let error_rate = errors / total_requests * 100.0;
log::info!("Error rate: {:.2}%", error_rate);
// Target: <0.1%

// Cloud costs
// DigitalOcean 4 CPU/8GB RAM = $96/month
// Track: CPU usage, memory usage, network egress
```

### Month 5: Customer Acquisition

**Pricing tiers**:
- **Free**: 1K events/month, 1 endpoint
- **Starter**: $29/month, 100K events, 10 endpoints
- **Pro**: $99/month, 1M events, unlimited endpoints
- **Enterprise**: Custom pricing, dedicated infrastructure

**Revenue targets**:
- Month 4: 10 free users (validation)
- Month 5: 5 paying customers = $145-495/month
- Month 6: 20 paying customers = $580-1,980/month

**Decision point**: If revenue > $500/month, C migration justified.

### Month 6: C Preparation

**Hire or train C expertise**:
- Modern C patterns (arena allocators, defer, Result types)
- libuv async programming
- PostgreSQL libpq
- Memory profiling (valgrind, AddressSanitizer)

**Study reference implementations**:
- NGINX source code (HTTP handling)
- Redis source code (event loop, memory management)
- HAProxy source code (load balancing)

---

## üöÄ PHASE 3: C Migration (MONTHS 7-10)

### Month 7: C Implementation

#### Week 1: Core Architecture
```c
// src/pipeline/main.c
#include <uv.h>
#include <libpq-fe.h>
#include <hiredis/async.h>

typedef struct {
    uv_loop_t *loop;
    struct mpsc_queue *event_queue;
    struct mpsc_queue *delivery_queue;
    PGconn *db_conn;
    struct arena *arena;
} pipeline_t;

int main(int argc, char **argv) {
    // Initialize
    pipeline_t pipeline = {0};
    pipeline.loop = uv_default_loop();
    pipeline.arena = arena_create(64 * 1024 * 1024); // 64MB arena
    
    // Setup defer cleanup
    defer(pipeline_cleanup) pipeline_t *p = &pipeline;
    
    // Connect to database
    Result db_result = db_connect(&pipeline);
    if (db_result.err) {
        log_error("Database connection failed: %s", db_result.err);
        return 1;
    }
    
    // Setup WebSocket listeners
    for (int i = 0; i < 4; i++) {
        setup_websocket_listener(&pipeline, chains[i]);
    }
    
    // Setup batch processor
    uv_prepare_t batch_processor;
    uv_prepare_init(pipeline.loop, &batch_processor);
    uv_prepare_start(&batch_processor, process_batch_cb);
    
    // Setup HTTP workers
    setup_http_workers(&pipeline, 50);
    
    // Run event loop
    log_info("Pipeline started");
    uv_run(pipeline.loop, UV_RUN_DEFAULT);
    
    return 0;
}  // Automatic cleanup via defer
```

#### Week 2: Memory Management (arena allocator)
```c
// src/common/arena.c (already implemented!)
typedef struct arena {
    uint8_t *base;
    size_t size;
    size_t cursor;
    size_t capacity;
    size_t num_allocations;
    size_t peak_usage;
} arena_t;

arena_t *arena_create(size_t capacity) {
    arena_t *arena = malloc(sizeof(arena_t));
    
    // Align capacity to page size
    size_t page_size = sysconf(_SC_PAGESIZE);
    capacity = ALIGN_UP(capacity, page_size);
    
    // Allocate with mmap (returns memory to OS)
    arena->base = mmap(
        NULL, capacity,
        PROT_READ | PROT_WRITE,
        MAP_PRIVATE | MAP_ANONYMOUS,
        -1, 0
    );
    
    arena->size = 0;
    arena->capacity = capacity;
    arena->cursor = 0;
    
    return arena;
}

void *arena_alloc(arena_t *arena, size_t size) {
    // Align to 8 bytes
    size = ALIGN_UP(size, 8);
    
    // Check capacity
    if (arena->cursor + size > arena->capacity) {
        log_error("Arena out of memory: %zu + %zu > %zu",
            arena->cursor, size, arena->capacity);
        return NULL;
    }
    
    // Bump allocation (super fast!)
    void *ptr = arena->base + arena->cursor;
    arena->cursor += size;
    arena->num_allocations++;
    
    if (arena->cursor > arena->peak_usage) {
        arena->peak_usage = arena->cursor;
    }
    
    return ptr;
}

void arena_destroy(arena_t *arena) {
    // Single mmap call frees everything!
    munmap(arena->base, arena->capacity);
    
    log_info("Arena destroyed: allocated=%zu, peak=%zu MB",
        arena->num_allocations,
        arena->peak_usage / 1024 / 1024);
    
    free(arena);
}
```

#### Week 3: Batch Processing
```c
// src/pipeline/batch.c
typedef struct {
    event_t *events;
    size_t count;
    size_t capacity;
} event_batch_t;

void process_batch_cb(uv_prepare_t *handle) {
    pipeline_t *pipeline = handle->data;
    event_batch_t *batch = &pipeline->batch;
    
    // Collect events from queue
    while (batch->count < 100) {
        event_t *event = mpsc_queue_try_pop(pipeline->event_queue);
        if (!event) break;
        
        batch->events[batch->count++] = *event;
    }
    
    if (batch->count == 0) return;
    
    // Process batch with SINGLE query
    arena_t *arena = arena_create(1024 * 1024);
    
    Result result = process_event_batch(
        pipeline->db_conn,
        batch->events,
        batch->count,
        arena
    );
    
    if (result.err) {
        log_error("Batch processing failed: %s", result.err);
    } else {
        log_info("Processed %zu events", batch->count);
    }
    
    // Reset batch
    batch->count = 0;
    arena_destroy(arena);
}

Result process_event_batch(
    PGconn *conn,
    event_t *events,
    size_t count,
    arena_t *arena
) {
    // Build SQL query
    char *query = arena_alloc(arena, 4096);
    size_t offset = snprintf(query, 4096,
        "SELECT DISTINCT e.* FROM endpoints e WHERE e.is_active = true AND ("
    );
    
    for (size_t i = 0; i < count; i++) {
        offset += snprintf(query + offset, 4096 - offset,
            "e.contract_address = '%s'%s",
            events[i].contract_address,
            (i < count - 1) ? " OR " : ""
        );
    }
    
    offset += snprintf(query + offset, 4096 - offset, ")");
    
    // Execute query
    PGresult *res = PQexec(conn, query);
    
    if (PQresultStatus(res) != PGRES_TUPLES_OK) {
        char *err = arena_alloc(arena, 256);
        snprintf(err, 256, "Query failed: %s", PQerrorMessage(conn));
        PQclear(res);
        return (Result){.err = err};
    }
    
    // Parse results
    int n_rows = PQntuples(res);
    endpoint_t *endpoints = arena_alloc(arena, sizeof(endpoint_t) * n_rows);
    
    for (int i = 0; i < n_rows; i++) {
        // Parse endpoint from row
        parse_endpoint_from_row(res, i, &endpoints[i], arena);
    }
    
    PQclear(res);
    
    // Create delivery jobs
    for (size_t i = 0; i < count; i++) {
        for (int j = 0; j < n_rows; j++) {
            if (matches_endpoint(&events[i], &endpoints[j])) {
                delivery_job_t *job = arena_alloc(arena, sizeof(delivery_job_t));
                job->event = events[i];
                job->endpoint = endpoints[j];
                
                mpsc_queue_push(pipeline->delivery_queue, job);
            }
        }
    }
    
    return (Result){.ok = (void*)1};
}
```

#### Week 4: HTTP Delivery (libcurl multi)
```c
// src/pipeline/delivery.c
#include <curl/curl.h>

typedef struct {
    CURLM *multi_handle;
    CURL *easy_handles[50];
    size_t num_workers;
    uv_timer_t timer;
} http_workers_t;

void setup_http_workers(pipeline_t *pipeline, size_t workers) {
    http_workers_t *hw = malloc(sizeof(http_workers_t));
    hw->multi_handle = curl_multi_init();
    hw->num_workers = workers;
    
    // Initialize easy handles
    for (size_t i = 0; i < workers; i++) {
        hw->easy_handles[i] = curl_easy_init();
        curl_easy_setopt(hw->easy_handles[i], CURLOPT_TIMEOUT, 30L);
    }
    
    // Setup timer for polling
    uv_timer_init(pipeline->loop, &hw->timer);
    hw->timer.data = hw;
    uv_timer_start(&hw->timer, http_worker_cb, 0, 10);  // Every 10ms
}

void http_worker_cb(uv_timer_t *timer) {
    http_workers_t *hw = timer->data;
    
    // Check for delivery jobs
    delivery_job_t *job = mpsc_queue_try_pop(pipeline->delivery_queue);
    if (!job) return;
    
    // Find available handle
    CURL *handle = NULL;
    for (size_t i = 0; i < hw->num_workers; i++) {
        // Check if handle is idle
        if (!is_handle_busy(hw->easy_handles[i])) {
            handle = hw->easy_handles[i];
            break;
        }
    }
    
    if (!handle) return;  // All workers busy
    
    // Setup request
    char *payload = json_serialize(&job->event);
    char *signature = generate_hmac(payload, job->endpoint.hmac_secret);
    
    struct curl_slist *headers = NULL;
    headers = curl_slist_append(headers, "Content-Type: application/json");
    
    char header_buf[256];
    snprintf(header_buf, sizeof(header_buf), 
        "X-Webhook-Signature: %s", signature);
    headers = curl_slist_append(headers, header_buf);
    
    curl_easy_setopt(handle, CURLOPT_URL, job->endpoint.url);
    curl_easy_setopt(handle, CURLOPT_POSTFIELDS, payload);
    curl_easy_setopt(handle, CURLOPT_HTTPHEADER, headers);
    
    // Add to multi handle
    curl_multi_add_handle(hw->multi_handle, handle);
    
    // Perform requests
    int still_running;
    curl_multi_perform(hw->multi_handle, &still_running);
    
    // Check for completed requests
    CURLMsg *msg;
    int msgs_in_queue;
    while ((msg = curl_multi_info_read(hw->multi_handle, &msgs_in_queue))) {
        if (msg->msg == CURLMSG_DONE) {
            CURL *handle = msg->easy_handle;
            long response_code;
            curl_easy_getinfo(handle, CURLINFO_RESPONSE_CODE, &response_code);
            
            if (response_code >= 200 && response_code < 300) {
                log_info("Webhook delivered: %ld", response_code);
            } else {
                log_error("Webhook failed: %ld", response_code);
            }
            
            curl_multi_remove_handle(hw->multi_handle, handle);
        }
    }
}
```

### Month 8: Testing & Optimization

**Performance benchmarks**:
```c
// Measure memory usage
size_t memory_usage = get_rss_kb();
assert(memory_usage < 30 * 1024);  // <30MB

// Measure latency
uint64_t start = uv_hrtime();
process_event_batch(...);
uint64_t elapsed = uv_hrtime() - start;
assert(elapsed < 5 * 1000000);  // <5ms

// Measure throughput
size_t events_processed = 0;
uint64_t start = uv_hrtime();
while (uv_hrtime() - start < 1000000000) {  // 1 second
    events_processed += process_batch();
}
assert(events_processed > 50000);  // >50K events/sec
```

**Memory leak detection**:
```bash
# Valgrind
valgrind --leak-check=full \
         --show-leak-kinds=all \
         --track-origins=yes \
         ./pipeline

# AddressSanitizer
gcc -fsanitize=address -g pipeline.c -o pipeline
./pipeline
```

### Month 9: A/B Testing

**Deploy C pipeline to 10% traffic**:
```nginx
# /etc/nginx/conf.d/ethhook.conf
upstream pipeline {
    server 127.0.0.1:3000 weight=1;  # Rust (90%)
    server 127.0.0.1:4000 weight=1;  # C (10%)
}

server {
    listen 80;
    
    location / {
        proxy_pass http://pipeline;
    }
}
```

**Compare metrics**:
| Metric | Rust | C | Winner |
|--------|------|---|---------|
| Memory | 80MB | 28MB | C (65% less) |
| Latency P95 | 8ms | 6ms | C (25% faster) |
| Throughput | 45K/s | 58K/s | C (29% more) |
| Binary size | 18MB | 3.8MB | C (79% smaller) |
| Cold start | 150ms | 42ms | C (72% faster) |
| Stability | 99.9% | 99.95% | C (better) |

### Month 10: Full Migration

**Gradual cutover**:
- Week 1: 25% C, 75% Rust
- Week 2: 50% C, 50% Rust
- Week 3: 75% C, 25% Rust
- Week 4: 100% C

**Decommission Rust services**:
```bash
# Stop Rust pipeline
docker compose -f docker-compose.unified.yml down pipeline

# Keep Rust admin API (not performance-critical)
# Keep Rust for future features (can coexist with C)
```

---

## üí∞ COST ANALYSIS

### Current (3 services)
- DigitalOcean 4 CPU/8GB RAM: **$96/month**
- Memory usage: 350MB
- Binary size: 60MB total

### After Rust Unified
- Same droplet: **$96/month**
- Memory usage: 80MB (77% reduction)
- Binary size: 18MB (70% reduction)
- **Can handle 5x more traffic**

### After C Migration
- DigitalOcean 2 CPU/4GB RAM: **$36/month** (62% savings!)
- Memory usage: 30MB (91% reduction vs original)
- Binary size: 4MB (93% reduction vs original)
- **Can handle 10x more traffic**

### ROI Calculation
**Savings**: $96/month - $36/month = **$60/month = $720/year**

**C development cost**: 3 months * $5K/month = $15K

**Break-even**: $15K / $720 = **21 months**

**But if traffic grows 10x**:
- Rust: Would need $480/month (5 droplets)
- C: Would need $72/month (2 droplets)
- **Savings**: $408/month = $4,896/year
- **Break-even**: $15K / $4,896 = **3 months**

---

## üìã SUMMARY CHECKLIST

### Phase 1: Rust (Weeks 1-3) ‚úÖ
- [ ] Day 1-2: Project setup, architecture
- [ ] Day 3-4: WebSocket integration
- [ ] Day 5-7: Batch processor (CRITICAL)
- [ ] Day 8-10: HTTP delivery workers
- [ ] Day 11-12: Circuit breaker, error handling
- [ ] Day 13-14: Integration tests
- [ ] Day 15-17: Production deployment
- [ ] Day 18-19: Monitoring, metrics
- [ ] Day 20-21: Gradual rollout (10% ‚Üí 100%)

**Success criteria**:
- ‚úÖ Latency: <5ms P95
- ‚úÖ Memory: <100MB
- ‚úÖ Throughput: >10K events/sec
- ‚úÖ Error rate: <0.1%
- ‚úÖ Zero production panics

### Phase 2: Validation (Months 4-6) ‚úÖ
- [ ] Month 4: Metrics collection
- [ ] Month 5: Customer acquisition (target: 5-20 paying)
- [ ] Month 6: C preparation (hire/train)

**Success criteria**:
- ‚úÖ Revenue: >$500/month (justifies C migration)
- ‚úÖ Uptime: >99.9%
- ‚úÖ Customer satisfaction: >4.5/5

### Phase 3: C Migration (Months 7-10) ‚úÖ
- [ ] Month 7: C implementation
- [ ] Month 8: Testing, optimization
- [ ] Month 9: A/B testing (10% traffic)
- [ ] Month 10: Full cutover (100% traffic)

**Success criteria**:
- ‚úÖ Memory: <35MB (vs 80MB Rust)
- ‚úÖ Latency: <5ms P95 (same or better)
- ‚úÖ Stability: >99.95%
- ‚úÖ Cost savings: >60% ($96 ‚Üí $36/month)

---

## üéØ NEXT STEPS (START NOW!)

```bash
# 1. Create branch
git checkout -b feature/unified-pipeline

# 2. Create project
cargo new --bin crates/pipeline

# 3. Copy this file for reference
cp IMPLEMENTATION_ROADMAP.md crates/pipeline/ROADMAP.md

# 4. Start with Day 1 tasks
cd crates/pipeline
# ... implement as per plan above
```

**Let's start implementing! Which day should we begin with?**
