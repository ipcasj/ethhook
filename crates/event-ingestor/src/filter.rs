/*!
 * Event Filter Module
 *
 * Manages contract addresses and event topics for efficient log filtering.
 * Fetches active endpoint filters from PostgreSQL and uses them to reduce
 * Alchemy API costs by 50-90%.
 *
 * ## Cost Optimization Strategy
 *
 * **Before**: Fetch ALL logs from ALL contracts (540K CUs/day)
 * **After**: Fetch ONLY logs matching active endpoints (27K-270K CUs/day)
 *
 * ## Implementation
 *
 * 1. Query SQLite for active endpoint filters
 * 2. Build unified filter set (addresses + topics)
 * 3. Use eth_getLogs with filters
 * 4. Refresh filters periodically (5 minutes)
 */

use anyhow::{Context, Result};
use serde::{Deserialize, Serialize};
use sqlx::SqlitePool;
use std::collections::HashSet;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{info, warn};

/// Filter configuration for eth_getLogs
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LogFilter {
    /// Contract addresses to monitor (empty = all contracts)
    pub addresses: Vec<String>,

    /// Event topic signatures to monitor (empty = all events)
    pub topics: Vec<String>,

    /// Chain IDs to monitor
    pub chain_ids: Vec<i32>,
}

impl LogFilter {
    /// Create empty filter (matches all events)
    pub fn empty() -> Self {
        Self {
            addresses: Vec::new(),
            topics: Vec::new(),
            chain_ids: Vec::new(),
        }
    }

    /// Check if filter is empty (would match all events)
    pub fn is_empty(&self) -> bool {
        self.addresses.is_empty() && self.topics.is_empty()
    }

    /// Get unique contract addresses for a specific chain
    pub fn addresses_for_chain(&self, chain_id: u64) -> Vec<String> {
        if self.chain_ids.is_empty() || self.chain_ids.contains(&(chain_id as i32)) {
            self.addresses.clone()
        } else {
            Vec::new()
        }
    }

    /// Get unique event topics
    pub fn topics(&self) -> Vec<String> {
        self.topics.clone()
    }
}

/// Filter manager that fetches and caches endpoint filters
pub struct FilterManager {
    /// SQLite connection pool
    pool: SqlitePool,

    /// Cached filter configuration
    filter: Arc<RwLock<LogFilter>>,

    /// Refresh interval in seconds
    refresh_interval_secs: u64,
}

impl FilterManager {
    /// Create new filter manager
    ///
    /// # Arguments
    ///
    /// * `pool` - SQLite connection pool
    /// * `refresh_interval_secs` - How often to refresh filters (default: 300 = 5 minutes)
    pub async fn new(pool: SqlitePool, refresh_interval_secs: u64) -> Result<Self> {
        let filter = Arc::new(RwLock::new(LogFilter::empty()));

        let manager = Self {
            pool,
            filter,
            refresh_interval_secs,
        };

        // Initial filter load
        manager.refresh_filters().await?;

        Ok(manager)
    }

    /// Get current filter configuration
    pub async fn get_filter(&self) -> LogFilter {
        self.filter.read().await.clone()
    }

    /// Refresh filters from database
    ///
    /// Queries SQLite for all active endpoints and builds unified filter.
    pub async fn refresh_filters(&self) -> Result<()> {
        info!("üîÑ Refreshing event filters from database...");

        // Query active endpoints with their filters
        let rows = sqlx::query!(
            r#"
            SELECT 
                contract_addresses,
                event_signatures,
                chain_ids
            FROM endpoints
            WHERE is_active = true
              AND (contract_addresses IS NOT NULL OR event_signatures IS NOT NULL)
            "#
        )
        .fetch_all(&self.pool)
        .await
        .context("Failed to fetch active endpoint filters")?;

        let mut addresses_set: HashSet<String> = HashSet::new();
        let mut topics_set: HashSet<String> = HashSet::new();
        let mut chain_ids_set: HashSet<i32> = HashSet::new();

        for row in rows {
            // Collect contract addresses
            if let Some(addrs) = row.contract_addresses {
                for addr in addrs {
                    // Normalize to lowercase for consistency
                    addresses_set.insert(addr.to_lowercase());
                }
            }

            // Collect event topics/signatures
            if let Some(topics) = row.event_signatures {
                for topic in topics {
                    topics_set.insert(topic.to_lowercase());
                }
            }

            // Collect chain IDs
            if let Some(chain_ids) = row.chain_ids {
                for chain_id in chain_ids {
                    chain_ids_set.insert(chain_id);
                }
            }
        }

        // Build filter
        let new_filter = LogFilter {
            addresses: addresses_set.into_iter().collect(),
            topics: topics_set.into_iter().collect(),
            chain_ids: chain_ids_set.into_iter().collect(),
        };

        info!(
            "‚úÖ Filters refreshed: {} addresses, {} topics, {} chains",
            new_filter.addresses.len(),
            new_filter.topics.len(),
            new_filter.chain_ids.len()
        );

        if new_filter.is_empty() {
            warn!("‚ö†Ô∏è  No active endpoint filters found! Will fetch ALL events (expensive)");
        }

        // Update cached filter
        *self.filter.write().await = new_filter;

        Ok(())
    }

    /// Start background task to periodically refresh filters
    ///
    /// Runs until shutdown signal received.
    pub async fn start_refresh_loop(self: Arc<Self>) {
        let interval = tokio::time::Duration::from_secs(self.refresh_interval_secs);

        info!(
            "üîÑ Starting filter refresh loop (interval: {}s)",
            self.refresh_interval_secs
        );

        loop {
            tokio::time::sleep(interval).await;

            if let Err(e) = self.refresh_filters().await {
                warn!("‚ùå Failed to refresh filters: {}", e);
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_empty_filter() {
        let filter = LogFilter::empty();
        assert!(filter.is_empty());
        assert_eq!(filter.addresses.len(), 0);
        assert_eq!(filter.topics.len(), 0);
    }

    #[test]
    fn test_filter_for_chain() {
        let filter = LogFilter {
            addresses: vec!["0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48".to_string()],
            topics: vec![
                "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef".to_string(),
            ],
            chain_ids: vec![1, 42161], // Ethereum + Arbitrum
        };

        // Should match Ethereum
        let addrs = filter.addresses_for_chain(1);
        assert_eq!(addrs.len(), 1);

        // Should match Arbitrum
        let addrs = filter.addresses_for_chain(42161);
        assert_eq!(addrs.len(), 1);

        // Should NOT match Optimism
        let addrs = filter.addresses_for_chain(10);
        assert_eq!(addrs.len(), 0);
    }
}
